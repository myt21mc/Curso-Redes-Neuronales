{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57ee78f6-a69c-4701-b833-ad21a519ed8a",
   "metadata": {},
   "source": [
    "## Tarea 6: Reconocimiento facial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78784605-9224-4b59-b3bf-0f56643543ca",
   "metadata": {},
   "source": [
    "Mytzi Yael Munguía Cuatlayotl\n",
    "\n",
    "El reconocimiento facial, en general tiene 3 etapas:\n",
    "\n",
    "1. Detección de rostros: Detectar los rostros y sus posiciones en una imagen.\n",
    "2. Alineación de los rostros: Selección de los rostros y su transformación para homogeneizar el tamaño de las imágenes de rostros\n",
    "3. Extracción de características\n",
    "\n",
    "Para facilitar este ejercicio, nosotros sólo nos concentraremos en la tarea de extracción de características. Es decir, nuestra red sólo será alimentada con fotos de rostros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be47fff-1756-4606-8a12-4ce7e91500ef",
   "metadata": {},
   "source": [
    "## 2. Segunda parte: Red para el reconocimiento facial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1be047-7aff-4dee-a3dd-bd2aca23e038",
   "metadata": {},
   "source": [
    "**Importamos las librerias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1356b632-325c-44c5-b7a2-699ddb97fa6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\REC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, Sequential\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras import layers, Sequential\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e9f1e11-aab6-43d8-87dd-87dd7e70b7d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Ahora puedes usar pd.read_csv y otras funciones de pandas\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_myt \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/REC/Desktop/javier/att_mytzi.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Crea un conjunto de datos de TensorFlow a partir de un DataFrame de Pandas\u001b[39;00m\n\u001b[0;32m      5\u001b[0m files \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(df_myt[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Obtiene la primera columna\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Ahora puedes usar pd.read_csv y otras funciones de pandas\n",
    "df_myt = pd.read_csv(\"C:/Users/REC/Desktop/javier/att_mytzi.csv\", sep=',', header=None)\n",
    "\n",
    "# Crea un conjunto de datos de TensorFlow a partir de un DataFrame de Pandas\n",
    "files = tf.data.Dataset.from_tensor_slices(df_myt[0])  # Obtiene la primera columna\n",
    "attributes = tf.data.Dataset.from_tensor_slices(df_myt.iloc[:, 1:].to_numpy())  # obtiene los atributos\n",
    "data = tf.data.Dataset.zip((files, attributes))  # Combina los dos anteriores\n",
    "\n",
    "# Imprime información sobre el conjunto de datos\n",
    "print(data)\n",
    "\n",
    "# Directorio que contiene las imágenes. PD: no olvidar el / al final de la ruta\n",
    "path_to_images =\"C:/Users/mytzi/Documents/Redes_Neuronales/Curso-Redes-Neuronales/Reconocimiento_facial/imagenes_mytzi/\"\n",
    "# Procesamiento de cada archivo del conjunto de datos\n",
    "def process_file(file_name, attributes):\n",
    "    image = tf.io.read_file(path_to_images + file_name)  # Leé el archivo de imagen\n",
    "    image = tf.image.decode_jpeg(image, channels=3)  # Decodifica la imagen\n",
    "    image = tf.image.resize(image, [192, 192])  # Cambia el tamaño de la imagen\n",
    "    image /= 255.0  # Normaliza los valores de píxeles al rango [0, 1] IMPORTANTE\n",
    "    return image, attributes\n",
    "\n",
    "# Aplica la función\n",
    "labeled_images = data.map(process_file)\n",
    "\n",
    "# Imprime información sobre el conjunto de datos procesado\n",
    "print(labeled_images)\n",
    "\n",
    "# Visualiza las dos primeras imágenes del conjunto de datos\n",
    "for image, attri in labeled_images.take(2):\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52cb1260-e763-4bea-87aa-b954ad6ac455",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Carga el modelo preentrenado para los atributos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m red_att\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/REC/Desktop/javier/att_mytzi.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m pretrained_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred_att\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Congela las capas convolucionales preentrenadas\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m pretrained_model\u001b[38;5;241m.\u001b[39mlayers:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Carga el modelo preentrenado para los atributos\n",
    "red_att=\"C:/Users/REC/Desktop/javier/att_mytzi.csv\"\n",
    "pretrained_model = load_model(\"red_att\")  \n",
    "\n",
    "# Congela las capas convolucionales preentrenadas\n",
    "for layer in pretrained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Crea un nuevo modelo con las capas convolucionales preentrenadas\n",
    "feature_extractor = Model(inputs=pretrained_model.input, outputs=pretrained_model.layers[-3].output)\n",
    "\n",
    "# Añadir un clasificador binario\n",
    "classifier = Sequential([\n",
    "    layers.Flatten(input_shape=feature_extractor.output_shape[1:]),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Combinar el extractor de características y el clasificador binario\n",
    "binary_model = Sequential([\n",
    "    feature_extractor,\n",
    "    classifier\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "binary_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#====================================================================================================================================\n",
    "\n",
    "df_myt = pd.read_csv(\"C:/Users/REC/Desktop/javier/att_mytzi.csv\", sep=',', header=None)\n",
    "\n",
    "# División del conjunto de datos\n",
    "train_data, test_data = train_test_split(df_myt, test_size=0.2)\n",
    "\n",
    "# Crea un conjunto de datos de TensorFlow a partir de un DataFrame de Pandas\n",
    "files = tf.data.Dataset.from_tensor_slices(df_myt[0])  # Obtiene la primera columna\n",
    "attributes = tf.data.Dataset.from_tensor_slices(df_myt.iloc[:, 1:].to_numpy())  # obtiene los atributos\n",
    "data = tf.data.Dataset.zip((files, attributes))  # Combina los dos anteriores\n",
    "\n",
    "# Imprime información sobre el conjunto de datos\n",
    "print(data)\n",
    "\n",
    "# Directorio que contiene las nuevas imágenes. PD: no olvidar el / al final de la ruta\n",
    "path_to_images =\"C:/Users/REC/Desktop/javier/imagenes_mytzi/\"\n",
    "\n",
    "# Configurar el generador de imágenes aumentadas\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\"\"\"\n",
    "# Procesamiento de cada archivo del conjunto de datos\n",
    "def process_file(file_name, attributes):\n",
    "    image = tf.io.read_file(path_to_images + file_name)  # Leé el archivo de imagen\n",
    "    image = tf.image.decode_jpeg(image, channels=3)  # Decodifica la imagen\n",
    "    image = tf.image.resize(image, [192, 192])  # Cambia el tamaño de la imagen\n",
    "    image /= 255.0  # Normaliza los valores de píxeles al rango [0, 1] IMPORTANTE\n",
    "    return image, attributes\n",
    "\n",
    "# Aplica la función\n",
    "labeled_images = data.map(process_file)\n",
    "\"\"\"\n",
    "\n",
    "# Función para cargar y procesar imágenes desde el conjunto de datos de TensorFlow\n",
    "def process_file(file_name, label, folder_path):\n",
    "    image = tf.io.read_file(os.path.join(folder_path, file_name))  # Lee el archivo de imagen\n",
    "    image = tf.image.decode_jpeg(image, channels=3)  # Decodifica la imagen\n",
    "    image = tf.image.resize(image, [192, 192])  # Cambia el tamaño de la imagen\n",
    "    image /= 255.0  # Normaliza los valores de píxeles al rango [0, 1]\n",
    "    return image, label, folder_path\n",
    "\n",
    "# Aplica la función al conjunto de datos de TensorFlow\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data['filename'], train_data['label'], train_data['folder_path']))\n",
    "train_dataset = train_dataset.map(lambda x, y, z: process_file(x, y, z))\n",
    "train_dataset = train_dataset.batch(32)\n",
    "\n",
    "# Generar imágenes aumentadas durante el entrenamiento\n",
    "augmented_train_dataset = train_dataset.map(lambda x, y, z: (datagen.random_transform(x), y, z))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data['filename'], test_data['label'], test_data['folder_path']))\n",
    "test_dataset = test_dataset.map(lambda x, y, z: process_file(x, y, z))\n",
    "test_dataset = test_dataset.batch(32)\n",
    "\n",
    "# Entrenar el modelo binario con las nuevas imágenes aumentadas\n",
    "binary_model.fit(augmented_train_dataset, epochs=10, validation_data=test_dataset)\n",
    "\n",
    "# Guardar el modelo binario para su uso posterior\n",
    "binary_model.save('modelo_binario.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c157e969-93da-4987-be2b-7e13ead0c169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
